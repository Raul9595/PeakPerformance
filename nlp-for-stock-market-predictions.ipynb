{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "import re\n",
    "  \n",
    "# for Stemming propose  \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "stock_data = pd.read_csv(\"input/DJIA_table.csv\") \n",
    "news_data = pd.read_csv(\"input/RedditNews.csv\") \n",
    "news_data = news_data.merge(stock_data, on='Date')\n",
    "\n",
    "train_news_data = news_data[news_data['Date'] < '2015-01-01']\n",
    "test_news_data = news_data[news_data['Date'] > '2014-12-31']\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "grouped_news = pd.DataFrame(columns=['Date','Stacked News','Open','High','Low','Close','Volume','Adj Close'])\n",
    "grouped_news['Date'] = train_news_data.groupby('Date')['Date'].apply(set)\n",
    "grouped_news['Stacked News'] = train_news_data.groupby('Date')['News'].apply(lambda x: x.sum())\n",
    "grouped_news['Open'] = train_news_data.groupby('Date')['Open'].unique()\n",
    "grouped_news['High'] = train_news_data.groupby('Date')['High'].unique()\n",
    "grouped_news['Low'] = train_news_data.groupby('Date')['Low'].unique()\n",
    "grouped_news['Close'] = train_news_data.groupby('Date')['Close'].unique()\n",
    "grouped_news['Volume'] = train_news_data.groupby('Date')['Volume'].unique()\n",
    "grouped_news['Adj Close'] = train_news_data.groupby('Date')['Adj Close'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the special characters\n",
    "grouped_news['Stacked News'] = [re.sub(r'\\W', ' ', row) for row in grouped_news['Stacked News']]\n",
    "\n",
    "# remove all single characters\n",
    "grouped_news['Stacked News'] = [re.sub(r'\\s+[a-zA-Z]\\s+', ' ', row) for row in grouped_news['Stacked News']]\n",
    "\n",
    "# Remove single characters from the start\n",
    "grouped_news['Stacked News'] = [re.sub(r'\\^[a-zA-Z]\\s+', ' ', row) for row in grouped_news['Stacked News']]\n",
    "\n",
    "# Substituting multiple spaces with single space\n",
    "grouped_news['Stacked News'] = [re.sub(r'\\s+', ' ', row, flags=re.I) for row in grouped_news['Stacked News']]\n",
    "\n",
    "# Removing prefixed 'b'\n",
    "grouped_news['Stacked News'] = [re.sub(r'^b\\s+', '', row) for row in grouped_news['Stacked News']]\n",
    "\n",
    "# Removing numbers\n",
    "grouped_news['Stacked News'] = [re.sub(r'\\d+', '', row) for row in grouped_news['Stacked News']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_news['Date'] = grouped_news['Date'].astype(str)\n",
    "grouped_news['Date'] = [row.strip(\"{}''\") for row in grouped_news['Date']]\n",
    "\n",
    "grouped_news['Open'] = grouped_news['Open'].astype(str)\n",
    "grouped_news['Open'] = [row.strip(\"[]\") for row in grouped_news['Open']]\n",
    "grouped_news['Open'] = grouped_news['Open'].astype(float)\n",
    "\n",
    "grouped_news['High'] = grouped_news['High'].astype(str)\n",
    "grouped_news['High'] = [row.strip(\"[]\") for row in grouped_news['High']]\n",
    "grouped_news['High'] = grouped_news['High'].astype(float)\n",
    "\n",
    "grouped_news['Low'] = grouped_news['Low'].astype(str)\n",
    "grouped_news['Low'] = [row.strip(\"[]\") for row in grouped_news['Low']]\n",
    "grouped_news['Low'] = grouped_news['Low'].astype(float)\n",
    "\n",
    "grouped_news['Close'] = grouped_news['Close'].astype(str)\n",
    "grouped_news['Close'] = [row.strip(\"[]\") for row in grouped_news['Close']]\n",
    "grouped_news['Close'] = grouped_news['Close'].astype(float)\n",
    "\n",
    "grouped_news['Volume'] = grouped_news['Volume'].astype(str)\n",
    "grouped_news['Volume'] = [row.strip(\"[]\") for row in grouped_news['Volume']]\n",
    "grouped_news['Volume'] = grouped_news['Volume'].astype(float)\n",
    "\n",
    "grouped_news['Adj Close'] = grouped_news['Adj Close'].astype(str)\n",
    "grouped_news['Adj Close'] = [row.strip(\"[]\") for row in grouped_news['Adj Close']]\n",
    "grouped_news['Adj Close'] = grouped_news['Adj Close'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_news['Stacked News'] = [row.lower() for row in grouped_news['Stacked News']]\n",
    "grouped_news['Vectorized Words'] = [CountVectorizer().build_tokenizer()(row) for row in grouped_news['Stacked News']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer() \n",
    "for itr,row in enumerate(grouped_news['Vectorized Words']):\n",
    "    a = ''\n",
    "    for inneritr, word in enumerate(row):\n",
    "        if not word in set(stopwords.words('english')):\n",
    "            a = a + ' ' + ps.stem(word)\n",
    "    grouped_news['Vectorized Words'].iloc[itr] = a\n",
    "grouped_news['Vectorized Words'] = [CountVectorizer().build_tokenizer()(row) for row in grouped_news['Vectorized Words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_news['Word Count'] = [[[x,data.count(x)] for x in set(data)] for data in grouped_news['Vectorized Words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_news['Increase/Decrease'] = grouped_news['Close'].diff()\n",
    "grouped_news.dropna(inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
